# MaddenRankings
For this project I decided to apply model to attempt to correctly predict the overall rating of the players in the EA Sports Madden 2019 Ultimate Team video game. I downloaded the open dataset from the online community website, Kaggle, and imported it to a Google Colab file for the project. The dataset gives the players characteristics such as name, team, position, height, and weight as well as their abilities, for example, speed, strength, and agility and combines these abilities to create a unique overall rating for each player. My model takes in all of a player’s abilities and uses it to calculate a player’s overall rating, which is the target of the prediction. After cleaning the data by removing unnecessary values and handling Nan values it was ready to be converted into testing and training data categories. In the project I perform prediction modeling on two different ratios of the testing and training data. The first ratio of testing and training data is 80% testing data and 20% training data. For the project I also wanted to see what would happen if we increased the amount of training data to 30% and only had 70% testing data. On these sets I used the Gaussian Naive Bayes Classifier and K-Nearest Neighbors Classifier model prediction algorithms from scikit-learn. For the K-Nearest Neighbors Classifier I used two different numbers for the hyperparameter n_neighbors to determine what impact increasing the number of neighbors did for the accuracy score of the algorithm. I ran the accuracy score from sklearn.metrics to compute the subset accuracy for each model prediction algorithm to determine which combination gave the most accurate results. The following table summarizes the performance of the models and different choice of hyper-parameters with results rounded to the nearest thousandth.
	                                        80% testing data and 20% training data                	70% testing data and 30% training data

Gaussian Naive Bayes Classifier	                0.691	                                                                    0.658

K-Nearest Neighbors Classifier
n_neighbors = 3	                                0.842	                                                                    0.806

K-Nearest Neighbors Classifier
n_neighbors = 6	                                0.801	                                                                    0.771

Based on the following results it is clear to see that overall working with models that use 80% of the data for testing and 20% of the data for training yields much better results. This makes sense because you have more data to test. Between the two models of prediction it is also clear to see that the K-Nearest Neighbors Classifier produced much more accurate results than the Gaussian Naive Bayes Classifier. This is most likely because the Gaussian Naive Bayes Classifier works better when using contiguous data and since the data had a range of 0-100, this is what led to bad performance. The K-Nearest Neighbors Classifier had good performance because the data had discrete outcomes because of the range of potential values. Between the two variations of n_neighbors the smaller number gave more accurate results, this is because the actual value of the data has a higher weight in what the model predicts. Overall, for this dataset using  the K-Nearest Neighbors Classifier with an 80/20 testing/training split gave the most accurate score.
